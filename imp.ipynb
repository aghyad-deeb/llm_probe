{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a_name = \"distilgpt2\"\n",
    "model_b_name = \"distilgpt2\"\n",
    "\n",
    "model_a = AutoModelForCausalLM.from_pretrained(model_a_name)\n",
    "tokenizer_a = AutoTokenizer.from_pretrained(model_a_name)\n",
    "model_b = AutoModelForCausalLM.from_pretrained(model_b_name)\n",
    "tokenizer_b = AutoTokenizer.from_pretrained(model_b_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 768, 6)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_dim_a = model_a.config.hidden_size\n",
    "hidden_dim_b = model_b.config.hidden_size\n",
    "hidden_dim_a, hidden_dim_b, model_a.config.num_hidden_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_a = 4\n",
    "layer_b = layer_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations_and_output(model, tokenizer, input: str, layer_idx):\n",
    "    tokens = tokenizer(input, return_tensors=\"pt\")\n",
    "    output = model(**tokens, output_hidden_states=True)\n",
    "    return output.hidden_states[layer_idx + 1], output # + 1 because embedding is at 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def modify_forward_function(model):\n",
    "    # Store original forward\n",
    "    original_forward = model.forward\n",
    "    \n",
    "    def new_forward(hidden_states=None, layer_idx=-1, attention_mask=None, **kwargs):\n",
    "        # If hidden states are provided, start from there\n",
    "        if hidden_states is not None:\n",
    "            # Run through remaining transformer layers\n",
    "            for i, block in enumerate(model.transformer.h[layer_idx + 1:]):\n",
    "                print(f\"{layer_idx=}\")\n",
    "                attention_mask = attention_mask.to(torch.bool)\n",
    "                layer_outputs = block(hidden_states, attention_mask=attention_mask)\n",
    "                hidden_states = layer_outputs[0]\n",
    "            \n",
    "            # Final layer norm\n",
    "            hidden_states = model.transformer.ln_f(hidden_states)\n",
    "            \n",
    "            # Language modeling head\n",
    "            lm_logits = model.lm_head(hidden_states)\n",
    "            \n",
    "            return lm_logits\n",
    "        \n",
    "        # Otherwise use original forward pass\n",
    "        return original_forward(attention_mask=attention_mask, **kwargs)\n",
    "    \n",
    "    # Replace the forward function\n",
    "    model.forward = new_forward\n",
    "\n",
    "# Apply the modification to model_b\n",
    "modify_forward_function(model_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations, output_a = get_activations_and_output(model_a, tokenizer_a, \"hi\", layer_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 50257])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_a.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from out layer_idx=4\n",
      "layer_idx=4\n"
     ]
    }
   ],
   "source": [
    "input = tokenizer_b(\"test\", return_tensors=\"pt\")\n",
    "output_b = model_b.forward(**input, hidden_states=activations, layer_idx=layer_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[True, True, True,  ..., True, True, True]]])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_a.logits == output_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"hi\"\n",
    "tokens = tokenizer_a(text, return_tensors=\"pt\")\n",
    "output = model_a(**tokens, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output.hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_a.config.num_hidden_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 768])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple\n",
    "from \n",
    "\n",
    "class Zombie:\n",
    "    def __init__(self, model_a_name, model_b_name, layer_a_idx, layer_b_idx, exit_layer_b=None, num_classes=2, project=False):\n",
    "        self.model_a = AutoModelForCausalLM.from_pretrained(model_a_name)\n",
    "        self.tokenizer_a = AutoTokenizer.from_pretrained(model_a_name)\n",
    "        self.model_b = AutoModelForCausalLM.from_pretrained(model_b_name)\n",
    "        self.tokenizer_b = AutoTokenizer.from_pretrained(model_b_name)\n",
    "        for param in self.model_a.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.model_b.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.layer_a_idx = layer_a_idx\n",
    "        self.layer_b_idx = layer_b_idx\n",
    "        self.exit_layer_b = exit_layer_b\n",
    "        self.classifier = torch.nn.Linear(model_b.config.hidden_size, num_classes)\n",
    "        self.projection = torch.nn.Linear(model_a.config.hidden_size, model_b.config.hidden_size) if project else torch.nn.Identity()\n",
    "        self.modify_forward_function(self.model_b)\n",
    "    \n",
    "    def get_activations_and_output(self, model, tokenizer, input: str, layer_idx):\n",
    "        tokens = tokenizer(input, return_tensors=\"pt\")\n",
    "        output = model(**tokens, output_hidden_states=True)\n",
    "        return output.hidden_states[layer_idx + 1], output # + 1 because embedding is at 0\n",
    "    \n",
    "    def modify_forward_function(self, model):\n",
    "        # Store original forward\n",
    "        original_forward = model.forward\n",
    "        \n",
    "        def new_forward(hidden_states=None, layer_idx=-1, attention_mask=None, **kwargs):\n",
    "            # If hidden states are provided, start from there\n",
    "            if hidden_states is not None:\n",
    "                output = dict()\n",
    "                # Run through remaining transformer layers\n",
    "                for i, block in enumerate(model.transformer.h[layer_idx + 1:]):\n",
    "                    print(f\"{layer_idx=}\")\n",
    "                    attention_mask = attention_mask.to(torch.bool)\n",
    "                    layer_outputs = block(hidden_states, attention_mask=attention_mask)\n",
    "                    hidden_states = layer_outputs[0]\n",
    "                    if i == self.exit_layer_b:\n",
    "                        break\n",
    "                if self.exit_layer_b != None:\n",
    "                    output[\"hidden_states\"] = hidden_states\n",
    "\n",
    "                hidden_states = model.transformer.ln_f(hidden_states)\n",
    "                \n",
    "                # Language modeling head\n",
    "                output[\"logits\"] = model.lm_head(hidden_states)\n",
    "                \n",
    "                return output\n",
    "            \n",
    "            # Otherwise use original forward pass\n",
    "            return original_forward(attention_mask=attention_mask, **kwargs)\n",
    "        \n",
    "        # Replace the forward function\n",
    "        model.forward = new_forward\n",
    "\n",
    "    def forward(self, input_text: str):\n",
    "        activations_a, output_a = self.get_activations_and_output(\n",
    "            self.model_a, self.tokenizer_a, input_text, self.layer_a_idx\n",
    "        )\n",
    "        input = self.tokenizer_b(input_text, return_tensors=\"pt\")\n",
    "        activations_a = self.projection(activations_a)\n",
    "        output_b = self.model_b.forward(attention_mask=input[\"attention_mask\"], hidden_states=activations_a, layer_idx=self.layer_b_idx)\n",
    "        classifier_output = self.classifier(output_b[\"hidden_states\"])\n",
    "        return classifier_output, output_b[\"logits\"], output_a\n",
    "    \n",
    "    def train(self, data: List[Tuple[str, bool]]):\n",
    "        batch_size = 4\n",
    "        for i in range(len(data) // batch_size):\n",
    "\n",
    "            loss = 0\n",
    "            for input, label in data[i: i + batch_size]:\n",
    "                pred, _, _ = self.forward(input)\n",
    "                loss += torch.nn.functional.binary_cross_entropy(pred, label)\n",
    "            loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = Zombie(\"distilgpt2\", \"distilgpt2\", 4, 4, exit_layer_b=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_idx=4\n"
     ]
    }
   ],
   "source": [
    "classifier_output, output_b, output_a = z.forward(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[9.9998e-01, 1.7292e-05]]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_output.softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 50257])"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.all(output_a.logits == output_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
