{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a_name = \"distilgpt2\"\n",
    "model_b_name = \"distilgpt2\"\n",
    "\n",
    "model_a = AutoModelForCausalLM.from_pretrained(model_a_name)\n",
    "tokenizer_a = AutoTokenizer.from_pretrained(model_a_name)\n",
    "model_b = AutoModelForCausalLM.from_pretrained(model_b_name)\n",
    "tokenizer_b = AutoTokenizer.from_pretrained(model_b_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 768, 6)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_dim_a = model_a.config.hidden_size\n",
    "hidden_dim_b = model_b.config.hidden_size\n",
    "hidden_dim_a, hidden_dim_b, model_a.config.num_hidden_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_a = 4\n",
    "layer_b = layer_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations_and_output(model, tokenizer, input: str, layer_idx):\n",
    "    tokens = tokenizer(input, return_tensors=\"pt\")\n",
    "    output = model(**tokens, output_hidden_states=True)\n",
    "    return output.hidden_states[layer_idx + 1], output # + 1 because embedding is at 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def modify_forward_function(model):\n",
    "    # Store original forward\n",
    "    original_forward = model.forward\n",
    "    \n",
    "    def new_forward(hidden_states=None, layer_idx=-1, attention_mask=None, **kwargs):\n",
    "        # If hidden states are provided, start from there\n",
    "        if hidden_states is not None:\n",
    "            # Run through remaining transformer layers\n",
    "            for i, block in enumerate(model.transformer.h[layer_idx + 1:]):\n",
    "                print(f\"{layer_idx=}\")\n",
    "                attention_mask = attention_mask.to(torch.bool)\n",
    "                layer_outputs = block(hidden_states, attention_mask=attention_mask)\n",
    "                hidden_states = layer_outputs[0]\n",
    "            \n",
    "            # Final layer norm\n",
    "            hidden_states = model.transformer.ln_f(hidden_states)\n",
    "            \n",
    "            # Language modeling head\n",
    "            lm_logits = model.lm_head(hidden_states)\n",
    "            \n",
    "            return lm_logits\n",
    "        \n",
    "        # Otherwise use original forward pass\n",
    "        return original_forward(attention_mask=attention_mask, **kwargs)\n",
    "    \n",
    "    # Replace the forward function\n",
    "    model.forward = new_forward\n",
    "\n",
    "# Apply the modification to model_b\n",
    "modify_forward_function(model_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations, output_a = get_activations_and_output(model_a, tokenizer_a, \"hi\", layer_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 50257])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_a.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from out layer_idx=4\n",
      "layer_idx=4\n"
     ]
    }
   ],
   "source": [
    "input = tokenizer_b(\"test\", return_tensors=\"pt\")\n",
    "output_b = model_b.forward(**input, hidden_states=activations, layer_idx=layer_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[True, True, True,  ..., True, True, True]]])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_a.logits == output_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"hi\"\n",
    "tokens = tokenizer_a(text, return_tensors=\"pt\")\n",
    "output = model_a(**tokens, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output.hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_a.config.num_hidden_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 768])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from typing import List, Dict, Tuple\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "\n",
    "class Zombie(torch.nn.Module):\n",
    "    def __init__(self, model_a_name, model_b_name, layer_a_frac, layer_b_frac, exit_layer_b=None, num_classes=2, project=False):\n",
    "        super().__init__()\n",
    "        self.model_a = AutoModelForCausalLM.from_pretrained(model_a_name)\n",
    "        self.tokenizer_a = AutoTokenizer.from_pretrained(model_a_name)\n",
    "        self.model_b = AutoModelForCausalLM.from_pretrained(model_b_name)\n",
    "        self.tokenizer_b = AutoTokenizer.from_pretrained(model_b_name)\n",
    "        for param in self.model_a.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.model_b.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.num_layers_a = self.model_a.config.num_hidden_layers\n",
    "        self.num_layers_b = self.model_b.config.num_hidden_layers\n",
    "        self.layer_a_idx = int(layer_a_frac * self.num_layers_a)\n",
    "        self.layer_b_idx = int(layer_b_frac * self.num_layers_b)\n",
    "        self.exit_layer_b = exit_layer_b\n",
    "        self.classifier = torch.nn.Linear(self.model_b.config.hidden_size, num_classes)\n",
    "        self.projection = torch.nn.Linear(self.model_a.config.hidden_size, self.model_b.config.hidden_size) if project else torch.nn.Identity()\n",
    "        self.modify_forward_function(self.model_b)\n",
    "    \n",
    "    def get_activations_and_output(self, model, tokenizer, input: str, layer_idx):\n",
    "        tokens = tokenizer(input, return_tensors=\"pt\")\n",
    "        output = model(**tokens, output_hidden_states=True)\n",
    "        return output.hidden_states[layer_idx + 1], output # + 1 because embedding is at 0\n",
    "    \n",
    "    def modify_forward_function(self, model):\n",
    "        # Store original forward\n",
    "        original_forward = model.forward\n",
    "        \n",
    "        def new_forward(hidden_states=None, layer_idx=-1, attention_mask=None, **kwargs):\n",
    "            # If hidden states are provided, start from there\n",
    "            if hidden_states is not None:\n",
    "                output = dict()\n",
    "                # Run through remaining transformer layers\n",
    "                for i, block in enumerate(model.transformer.h[layer_idx + 1:]):\n",
    "                    # print(f\"{layer_idx=}\")\n",
    "                    attention_mask = attention_mask.to(torch.bool)\n",
    "                    layer_outputs = block(hidden_states, attention_mask=attention_mask)\n",
    "                    hidden_states = layer_outputs[0]\n",
    "                    if i == self.exit_layer_b:\n",
    "                        break\n",
    "                if self.exit_layer_b != None:\n",
    "                    output[\"hidden_states\"] = hidden_states\n",
    "\n",
    "                hidden_states = model.transformer.ln_f(hidden_states)\n",
    "                \n",
    "                # Language modeling head\n",
    "                output[\"logits\"] = model.lm_head(hidden_states)\n",
    "                \n",
    "                return output\n",
    "            \n",
    "            # Otherwise use original forward pass\n",
    "            return original_forward(attention_mask=attention_mask, **kwargs)\n",
    "        \n",
    "        # Replace the forward function\n",
    "        model.forward = new_forward\n",
    "\n",
    "    def forward(self, input_text: str):\n",
    "        activations_a, output_a = self.get_activations_and_output(\n",
    "            self.model_a, self.tokenizer_a, input_text, self.layer_a_idx\n",
    "        )\n",
    "        input = self.tokenizer_b(input_text, return_tensors=\"pt\")\n",
    "        activations_a = self.projection(activations_a)\n",
    "        output_b = self.model_b.forward(attention_mask=input[\"attention_mask\"], hidden_states=activations_a, layer_idx=self.layer_b_idx)\n",
    "        classifier_output = self.classifier(output_b[\"hidden_states\"])\n",
    "        return classifier_output, output_b[\"logits\"], output_a\n",
    "    \n",
    "    def train(self, data: List[Tuple[str, bool]]):\n",
    "        optim = torch.optim.AdamW(self.parameters())\n",
    "        batch_size = 8\n",
    "        for i in tqdm(range(len(data) // batch_size)):\n",
    "            loss = 0\n",
    "            for input, label in data[i: i + batch_size]:\n",
    "                logits, _, _ = self.forward(input)\n",
    "                pred = torch.nn.functional.softmax(logits, dim=-1)[..., label]\n",
    "                # pred = logits[..., label]\n",
    "                # label = torch.tensor(label)\n",
    "                # print(f\"{preds=}\")\n",
    "                # assert False\n",
    "                # print(f\"{pred=}\\n\\n{label=}\")\n",
    "                # assert False\n",
    "                loss += torch.nn.functional.binary_cross_entropy(pred, torch.ones_like(pred))\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "            print(f\"{loss=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Zombie.__init__() got an unexpected keyword argument 'layer_a_index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[104], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[43mZombie\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdistilgpt2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdistilgpt2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_a_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_b_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexit_layer_b\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: Zombie.__init__() got an unexpected keyword argument 'layer_a_index'"
     ]
    }
   ],
   "source": [
    "z = Zombie(\"distilgpt2\", \"distilgpt2\", layer_a_index=4, layer_b_index=2, exit_layer_b=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_output, output_b, output_a = z.forward(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1781, 0.8219]]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_output.softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 50257])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.all(output_a.logits == output_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 1),\n",
       " ('0', 0),\n",
       " ('1', 1),\n",
       " ('1', 1),\n",
       " ('0', 0),\n",
       " ('0', 0),\n",
       " ('0', 0),\n",
       " ('0', 0),\n",
       " ('1', 1),\n",
       " ('0', 0)]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "a = [(\"1\", 1)] * 1000\n",
    "b = [(\"0\", 0)] * 1000\n",
    "data = a + b\n",
    "random.shuffle(data)\n",
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09c21e2354454c1d8f41be25525fbcd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=tensor(0.0371, grad_fn=<AddBackward0>)\n",
      "loss=tensor(8.3252, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0712, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.5716, grad_fn=<AddBackward0>)\n",
      "loss=tensor(8.0531, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.9234, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0439, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0559, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.4610, grad_fn=<AddBackward0>)\n",
      "loss=tensor(1.2356, grad_fn=<AddBackward0>)\n",
      "loss=tensor(2.0672, grad_fn=<AddBackward0>)\n",
      "loss=tensor(1.7238, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.7695, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.3250, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0769, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0216, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0141, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0227, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0408, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0918, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.1209, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.2044, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.2848, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.4768, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.3846, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.3220, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.2064, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0952, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0437, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0213, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0133, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0086, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0102, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0144, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0213, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0381, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0523, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0535, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0636, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0528, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0369, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0371, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0361, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0340, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0312, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0280, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0369, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0423, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0354, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0291, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0238, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0149, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0124, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0134, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0113, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0098, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0100, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0099, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0094, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0086, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0072, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0065, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0061, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0058, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0063, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0072, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0075, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0089, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0093, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0096, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0112, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0128, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0114, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0113, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0110, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0094, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0079, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0066, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0054, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0044, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0045, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0036, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0045, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0054, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0062, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0069, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0069, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0075, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0081, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0079, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0077, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0075, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0068, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0067, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0065, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0064, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0063, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0063, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0061, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0061, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0059, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0058, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0057, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0056, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0059, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0059, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0059, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0059, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0064, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0064, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0064, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0070, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0064, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0064, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0058, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0064, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0058, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0063, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0058, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0058, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0063, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0062, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0066, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0066, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0068, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0064, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0066, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0062, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0059, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0059, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0058, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0058, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0058, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0058, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0058, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0058, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0057, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0056, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0056, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0056, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0055, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0054, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0054, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0053, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0053, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0056, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0052, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0056, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0061, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0062, grad_fn=<AddBackward0>)\n",
      "loss=tensor(0.0056, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[110], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[103], line 77\u001b[0m, in \u001b[0;36mZombie.train\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     75\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28minput\u001b[39m, label \u001b[38;5;129;01min\u001b[39;00m data[i: i \u001b[38;5;241m+\u001b[39m batch_size]:\n\u001b[0;32m---> 77\u001b[0m     logits, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, label]\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# pred = logits[..., label]\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m# label = torch.tensor(label)\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;66;03m# print(f\"{preds=}\")\u001b[39;00m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;66;03m# assert False\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;66;03m# print(f\"{pred=}\\n\\n{label=}\")\u001b[39;00m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;66;03m# assert False\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[103], line 62\u001b[0m, in \u001b[0;36mZombie.forward\u001b[0;34m(self, input_text)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_text: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m---> 62\u001b[0m     activations_a, output_a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_activations_and_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_a_idx\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer_b(input_text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     66\u001b[0m     activations_a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojection(activations_a)\n",
      "Cell \u001b[0;32mIn[103], line 26\u001b[0m, in \u001b[0;36mZombie.get_activations_and_output\u001b[0;34m(self, model, tokenizer, input, layer_idx)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_activations_and_output\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, tokenizer, \u001b[38;5;28minput\u001b[39m: \u001b[38;5;28mstr\u001b[39m, layer_idx):\n\u001b[1;32m     25\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m tokenizer(\u001b[38;5;28minput\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\u001b[38;5;241m.\u001b[39mhidden_states[layer_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m], output\n",
      "File \u001b[0;32m~/Documents/Coding/transformer_from_scratch/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Coding/transformer_from_scratch/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Coding/transformer_from_scratch/venv/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py:1443\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1440\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mset_device(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[1;32m   1441\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1443\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1445\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1447\u001b[0m     \u001b[38;5;66;03m# move labels to correct device to enable model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Coding/transformer_from_scratch/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Coding/transformer_from_scratch/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Coding/transformer_from_scratch/venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "z.train(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
